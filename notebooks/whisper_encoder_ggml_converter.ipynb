{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9xSJ_v5zHz3"
      },
      "outputs": [],
      "source": [
        "# This is the only setting you need to change.\n",
        "#Set this to the actual Whisper model's name that you want to convert:\n",
        "# tiny, base, small, medium, large for multilingual or *.en for English-only models\n",
        "MODEL_NAME = \"tiny\"\n",
        "\n",
        "# Model files URLs are here (in case there's a need to update in the future):\n",
        "# https://github.com/openai/whisper/blob/main/whisper/__init__.py#L17-L27\n",
        "_MODELS = {\n",
        "    \"tiny.en\": \"https://openaipublic.azureedge.net/main/whisper/models/d3dd57d32accea0b295c96e26691aa14d8822fac7d9d27d5dc00b4ca2826dd03/tiny.en.pt\",\n",
        "    \"tiny\": \"https://openaipublic.azureedge.net/main/whisper/models/65147644a518d12f04e32d6f3b26facc3f8dd46e5390956a9424a650c0ce22b9/tiny.pt\",\n",
        "    \"base.en\": \"https://openaipublic.azureedge.net/main/whisper/models/25a8566e1d0c1e2231d1c762132cd20e0f96a85d16145c3a00adf5d1ac670ead/base.en.pt\",\n",
        "    \"base\": \"https://openaipublic.azureedge.net/main/whisper/models/ed3a0b6b1c0edf879ad9b11b1af5a0e6ab5db9205f891f668f8b0e6c6326e34e/base.pt\",\n",
        "    \"small.en\": \"https://openaipublic.azureedge.net/main/whisper/models/f953ad0fd29cacd07d5a9eda5624af0f6bcf2258be67c92b79389873d91e0872/small.en.pt\",\n",
        "    \"small\": \"https://openaipublic.azureedge.net/main/whisper/models/9ecf779972d90ba49c06d968637d720dd632c55bbf19d441fb42bf17a411e794/small.pt\",\n",
        "    \"medium.en\": \"https://openaipublic.azureedge.net/main/whisper/models/d7440d1dc186f76616474e0ff0b3b6b879abc9d1a4926b7adfa41db2d497ab4f/medium.en.pt\",\n",
        "    \"medium\": \"https://openaipublic.azureedge.net/main/whisper/models/345ae4da62f9b3d59415adc60127b97c714f32e89e936602e85993674d08dcb1/medium.pt\",\n",
        "    \"large-v1\": \"https://openaipublic.azureedge.net/main/whisper/models/e4b87e7e0bf463eb8e6956e646f1e277e901512310def2c24bf0e11bd3c28e9a/large-v1.pt\",\n",
        "    \"large-v2\": \"https://openaipublic.azureedge.net/main/whisper/models/81f7c96c852ee8fc832187b0132e569d6c3065a3252ed18e56effd0b6a73e524/large-v2.pt\",\n",
        "    \"large\": \"https://openaipublic.azureedge.net/main/whisper/models/81f7c96c852ee8fc832187b0132e569d6c3065a3252ed18e56effd0b6a73e524/large-v2.pt\",\n",
        "}\n",
        "MODEL_URL = _MODELS[MODEL_NAME]\n",
        "\n",
        "MEL_FILTERS_URL = \"https://github.com/openai/whisper/raw/main/whisper/assets/mel_filters.npz\"\n",
        "\n",
        "\n",
        "import struct\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "import numpy as np\n",
        "\n",
        "# This is where the result will be saved - download it when ready\n",
        "FNAME_OUT = Path(f\"./ggml-model-{MODEL_NAME}.bin\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!rm -f $f\"{MODEL_NAME}.pt\"\n",
        "!rm mel_filters.npz\n",
        "!wget $MODEL_URL\n",
        "!wget $MEL_FILTERS_URL"
      ],
      "metadata": {
        "id": "wqVFTAGw0RTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model checkpoint\n",
        "checkpoint = torch.load(f\"{MODEL_NAME}.pt\", map_location=\"cpu\")\n",
        "\n",
        "# Filter out params that don't pertain to the encoder\n",
        "enc_pars = ('n_mels', 'n_audio_ctx', 'n_audio_state',\n",
        "            'n_audio_head', 'n_audio_layer')\n",
        "params = {p : checkpoint[\"dims\"][p] for p in enc_pars}\n",
        "\n",
        "# Filter out blocks that don't pertain to the encoder\n",
        "encoder = {k:v for k,v in checkpoint[\"model_state_dict\"].items()\n",
        "            if k.startswith('encoder.') }\n",
        "\n",
        "# Load MEL filters\n",
        "with np.load(\"mel_filters.npz\", allow_pickle=True) as f:\n",
        "  filters = f[f'mel_{params[\"n_mels\"]}']\n"
      ],
      "metadata": {
        "id": "9nRYG3qFJ9g6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfII253HrYK7",
        "outputId": "91e875b2-ce78-4161-d538-7a7a5e5b7cf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'n_mels': 80,\n",
              " 'n_audio_ctx': 1500,\n",
              " 'n_audio_state': 384,\n",
              " 'n_audio_head': 6,\n",
              " 'n_audio_layer': 4}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fout = FNAME_OUT.open(\"wb\")\n",
        "\n",
        "fout.write(struct.pack(\"i\", 0x67676d6c)) # magic: ggml in hex\n",
        "\n",
        "for k in (\n",
        "          'n_audio_ctx',\n",
        "          'n_audio_state',\n",
        "          'n_audio_head',\n",
        "          'n_audio_layer',\n",
        "          'n_mels',\n",
        "          ):\n",
        "  v = params.get(k)\n",
        "  fout.write(struct.pack(\"i\", v))\n",
        "fout.write(struct.pack(\"i\", True)) # signifies the use of float16\n",
        "\n",
        "# write mel filters\n",
        "fout.write(struct.pack(\"i\", filters.shape[0]))\n",
        "fout.write(struct.pack(\"i\", filters.shape[1]))\n",
        "for i in range(filters.shape[0]):\n",
        "    for j in range(filters.shape[1]):\n",
        "        fout.write(struct.pack(\"f\", filters[i][j]))\n",
        "\n",
        "# write model blocks\n",
        "for name in encoder.keys():\n",
        "    data = encoder[name].squeeze().numpy()\n",
        "    print(\"Processing variable: \" , name ,  \" with shape: \", data.shape)\n",
        "\n",
        "    # reshape conv bias from [n] to [n, 1]\n",
        "    if name in [\"encoder.conv1.bias\", \"encoder.conv2.bias\"]:\n",
        "        data = data.reshape(data.shape[0], 1)\n",
        "        print(f\"  Reshaped variable: {name} to shape: \", data.shape)\n",
        "\n",
        "    n_dims = len(data.shape)\n",
        "\n",
        "    # looks like the whisper models are in f16 by default\n",
        "    # so we need to convert the small tensors to f32 until we fully support f16 in ggml\n",
        "    # ftype == 0 -> float32, ftype == 1 -> float16\n",
        "    ftype = 1\n",
        "    if n_dims < 2 or \\\n",
        "            name == \"encoder.conv1.bias\"   or \\\n",
        "            name == \"encoder.conv2.bias\"   or \\\n",
        "            name == \"encoder.positional_embedding\" or \\\n",
        "            name == \"decoder.positional_embedding\":\n",
        "        print(\"  Converting to float32\")\n",
        "        data = data.astype(np.float32)\n",
        "        ftype = 0\n",
        "\n",
        "    str_ = name.encode('utf-8')\n",
        "    fout.write(struct.pack(\"iii\", n_dims, len(str_), ftype))\n",
        "    for i in range(n_dims):\n",
        "        fout.write(struct.pack(\"i\", data.shape[n_dims - 1 - i]))\n",
        "    fout.write(str_)\n",
        "\n",
        "    # data\n",
        "    data.tofile(fout)\n",
        "\n",
        "fout.close()\n"
      ],
      "metadata": {
        "id": "TLdmOa4p02an"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}